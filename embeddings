#!python3

import pickle
import re
import sqlite3

import sentence_transformers
import torch
from tqdm import tqdm
import sqlite_vec
from sqlite_vec import serialize_float32

from utils import Article

# Connect to the SQLite database
conn = sqlite3.connect("news.db")
cur = conn.cursor()

# cur.execute("SELECT COUNT(*) FROM articles_fts;")
# total = cur.fetchone()[0]
total = 100
cur.execute("SELECT title, text, uid FROM articles_fts LIMIT 500;")
all = cur.fetchall()
old_articles = [Article(res[0], res[1], res[2]) for res in all]
all = None
del all
new_articles = []
skipped = 0
for i, art in tqdm(enumerate(old_articles)):
    if art.body == "":
        skipped += 1
        continue
    elif len(re.findall(r"\w+", art.body)) < 200:
        skipped += 1
        continue
    else:
        art.body = re.sub(r"[\s]?{.*?}", "", art.body)
        new_articles.append(art)

old_articles = None
del old_articles

print(f"Skipped {skipped} articles")

conn.close()
conn = sqlite3.connect("search.db")
cur = conn.cursor()

deduped = 0
# Check for duplicates
try:
    for article in new_articles:
        cur.execute("SELECT * FROM articles WHERE uid = ?", (article.id,))
        if cur.fetchone() is not None:
            new_articles.remove(article)
            deduped += 1
except sqlite3.OperationalError:
    pass

conn.close()

print(f"Deduped {deduped} articles")
print(f"Generating embeddings for {len(new_articles)} articles")
# Load the pre-trained model
torch.device("mps")
model = sentence_transformers.SentenceTransformer("Snowflake/snowflake-arctic-embed-m")
embeddings = model.encode(
    [article.body for article in new_articles], show_progress_bar=True, batch_size=32
)
model = None
del model

for i, embed in enumerate(embeddings):
    new_articles[i].embedding = embed

conn = sqlite3.connect("news.db")
cur = conn.cursor()

# Fetch all metadata in one go
cur.execute("SELECT uid, date, url, publication FROM articles_meta WHERE uid IN ({})".format(
    ",".join("?" for _ in new_articles)
), [article.id for article in new_articles])

# Build a lookup dictionary
metadata = {row[0]: row[1:] for row in cur.fetchall()}

# Assign metadata to articles
for article in tqdm(new_articles, desc="Metadata"):
    date, url, publication = metadata.get(article.id, (None, None, None))
    article.date = date
    article.url = url
    article.publication = publication

conn.close()

# Save the new articles
conn = sqlite3.connect("search.db")
cur = conn.cursor()

conn.enable_load_extension(True)
sqlite_vec.load(conn)
conn.enable_load_extension(False)

cur.execute("CREATE TABLE IF NOT EXISTS articles (uid TEXT PRIMARY KEY, title TEXT, body TEXT, date TEXT, url TEXT, publication TEXT);")
cur.execute("CREATE VIRTUAL TABLE IF NOT EXISTS articles_fts USING fts5(uid, title, body, tokenize = 'porter unicode61 remove_diacritics 1');")
cur.execute("CREATE VIRTUAL TABLE IF NOT EXISTS vec_articles USING vec0(uid TEXT PRIMARY KEY, embedding FLOAT[768]);")
conn.commit()

for article in tqdm(new_articles, desc="Inserting articles"):
    try:
        cur.execute(
            "INSERT INTO articles (uid, title, body, date, url, publication) VALUES (?, ?, ?, ?, ?, ?);",
            (
                article.id,
                article.title,
                article.body,
                article.date,
                article.url,
                article.publication,
            ),
        )
        cur.execute(
            "INSERT INTO articles_fts (uid, title, body) VALUES (?, ?, ?);",
            (article.id, article.title, article.body),
        )
        cur.execute(
            "INSERT INTO vec_articles (uid, embedding) VALUES (?, ?);",
            (article.id, serialize_float32(article.embedding)),
        )
    except sqlite3.IntegrityError:
        continue

conn.commit()
conn.close()
