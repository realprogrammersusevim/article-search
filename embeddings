#!python3

import pickle
import re
import sqlite3

import sentence_transformers
import torch
from tqdm import tqdm
import sqlite_vec
from sqlite_vec import serialize_float32

from utils import Article

# Connect to the SQLite database
conn = sqlite3.connect("news.db")
cur = conn.cursor()

total = cur.execute("SELECT COUNT(*) FROM articles_fts;").fetchone()[0]
res = cur.execute("SELECT title, text, uid FROM articles_fts;")
new_articles = []
skipped = 0
for i, art in tqdm(enumerate(res), total=total, desc="Processing articles"):
    article = Article(art[0], art[1], art[2])
    if article.body == "":
        skipped += 1
        continue
    elif len(re.findall(r"\w+", article.body)) < 300:
        skipped += 1
        continue
    else:
        article.body = re.sub(r"[\s]?{.*?}", "", article.body)
        new_articles.append(article)

print(f"Skipped {skipped} articles")

conn.close()
conn = sqlite3.connect("search.db")
cur = conn.cursor()

deduped = 0
# Check for duplicates
for article in tqdm(new_articles, desc="Deduplicating"):
    try:
        cur.execute("SELECT * FROM articles WHERE uid = ?", (article.id,))
        if cur.fetchone() is not None:
            new_articles.remove(article)
            deduped += 1
    except sqlite3.OperationalError:
        pass

conn.close()

print(f"Deduped {deduped} articles")
print(f"Generating embeddings for {len(new_articles)} articles")
# Load the pre-trained model
torch.device("mps")
model = sentence_transformers.SentenceTransformer("Snowflake/snowflake-arctic-embed-m")
embeddings = model.encode(
    [article.body for article in new_articles], show_progress_bar=True, batch_size=32
)
model = None
del model

for i, embed in enumerate(embeddings):
    new_articles[i].embedding = embed

conn = sqlite3.connect("news.db")
cur = conn.cursor()

# Fetch all metadata in one go
cur.execute("SELECT uid, date, url, publication FROM articles_meta WHERE uid IN ({})".format(
    ",".join("?" for _ in new_articles)
), [article.id for article in new_articles])

# Build a lookup dictionary
metadata = {row[0]: row[1:] for row in cur.fetchall()}

# Assign metadata to articles
for article in tqdm(new_articles, desc="Metadata"):
    date, url, publication = metadata.get(article.id, (None, None, None))
    article.date = date
    article.url = url
    article.publication = publication

conn.close()

# Save the new articles
conn = sqlite3.connect("search.db")
cur = conn.cursor()

conn.enable_load_extension(True)
sqlite_vec.load(conn)
conn.enable_load_extension(False)

cur.execute("CREATE TABLE IF NOT EXISTS articles (uid TEXT PRIMARY KEY, title TEXT, body TEXT, date TEXT, url TEXT, publication TEXT);")
cur.execute("CREATE VIRTUAL TABLE IF NOT EXISTS articles_fts USING fts5(uid, title, body, tokenize = 'porter unicode61 remove_diacritics 1');")
cur.execute("CREATE VIRTUAL TABLE IF NOT EXISTS vec_articles USING vec0(uid TEXT PRIMARY KEY, embedding FLOAT[768]);")
conn.commit()

for article in tqdm(new_articles, desc="Inserting articles"):
    try:
        cur.execute(
            "INSERT INTO articles (uid, title, body, date, url, publication) VALUES (?, ?, ?, ?, ?, ?);",
            (
                article.id,
                article.title,
                article.body,
                article.date,
                article.url,
                article.publication,
            ),
        )
        cur.execute(
            "INSERT INTO articles_fts (uid, title, body) VALUES (?, ?, ?);",
            (article.id, article.title, article.body),
        )
        cur.execute(
            "INSERT INTO vec_articles (uid, embedding) VALUES (?, ?);",
            (article.id, serialize_float32(article.embedding)),
        )
    except sqlite3.IntegrityError:
        continue

conn.commit()
conn.close()
